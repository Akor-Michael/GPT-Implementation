{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdvmJHxcmMVI",
        "outputId": "16d32cda-e450-458d-ff07-662fad614df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mJuJ0U0WmEIG",
        "outputId": "826ca8b8-78f6-4e10-ede6-9ca0db382160"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'input (1).txt'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wget\n",
        "\n",
        "# Dataset URL\n",
        "dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "# Download the dataset from its URL and save it in input.txt\n",
        "wget.download(dataset_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M2zG9wnmEK7"
      },
      "outputs": [],
      "source": [
        "# Open the 'input.txt' file for reading with UTF-8 encoding\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    # Read the contents of the file into the 'text' variable\n",
        "    text = file.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7x_Slr8mENV",
        "outputId": "5c250852-587a-4d3e-97f1-75e360328a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oesceCRimEPy",
        "outputId": "8b63d27f-ba24-4565-ffa7-a204ab7a2577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary Size: 65\n"
          ]
        }
      ],
      "source": [
        "# Find all unique characters in the 'text' variable and sort them\n",
        "unique_characters = sorted(list(set(text)))\n",
        "\n",
        "# Calculate the vocabulary size, which is the total number of unique characters\n",
        "vocab_size = len(unique_characters)\n",
        "\n",
        "# Print the unique characters as a single string and the vocabulary size\n",
        "print('Unique Characters:', ''.join(unique_characters))\n",
        "print('Vocabulary Size:', vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmgg87IsmEST",
        "outputId": "343d8bf4-ce30-4687-99a8-6706d9c7f7b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoded: hii there\n"
          ]
        }
      ],
      "source": [
        "# Create a mapping from characters to integers using dictionary comprehension\n",
        "# stoi: character to integer mapping\n",
        "stoi = {ch: i for i, ch in enumerate(unique_characters)}\n",
        "\n",
        "# Create a mapping from integers to characters using dictionary comprehension\n",
        "# itos: integer to character mapping\n",
        "itos = {i: ch for i, ch in enumerate(unique_characters)}\n",
        "\n",
        "# Define an encoder function that takes a string and outputs a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# Define a decoder function that takes a list of integers and outputs a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Test the encoder and decoder functions\n",
        "encoded_text = encode(\"hii there\")\n",
        "decoded_text = decode(encoded_text)\n",
        "\n",
        "# Print the encoded and decoded results\n",
        "print(\"Encoded:\", encoded_text)\n",
        "print(\"Decoded:\", decoded_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKvmmJtLmEU5",
        "outputId": "56d03ce4-8e63-430f-d12e-200cb6c61534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded Data Shape: torch.Size([1115394])\n",
            "Encoded Data Type: torch.int64\n"
          ]
        }
      ],
      "source": [
        "# Import the PyTorch library for working with tensors\n",
        "import torch\n",
        "\n",
        "# Encode the entire text dataset using the 'encode' function defined earlier\n",
        "encoded_data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Print the shape and data type of the encoded data tensor\n",
        "print(\"Encoded Data Shape:\", encoded_data.shape)\n",
        "print(\"Encoded Data Type:\", encoded_data.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCnHTeX3mEZz"
      },
      "outputs": [],
      "source": [
        "# Calculate the index to split the data into train and validation sets (first 90% for train)\n",
        "n = int(0.9 * len(encoded_data))\n",
        "\n",
        "# Create the training data by taking the first 90% of the encoded data\n",
        "train_data = encoded_data[:n]\n",
        "\n",
        "# Create the validation data by taking the remaining 10% of the encoded data\n",
        "val_data = encoded_data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx_iEltzmEcf"
      },
      "outputs": [],
      "source": [
        "# Define the block size, which is the length of the sequence to be extracted\n",
        "block_size = 8\n",
        "\n",
        "# Extract a sequence of characters from the beginning of the training data\n",
        "sequence = train_data[:block_size + 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL6qkSX7mEe_",
        "outputId": "0a43233b-030e-4e63-f3b3-70dd271b53ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When input is tensor([18]), the target is: 47\n",
            "When input is tensor([18, 47]), the target is: 56\n",
            "When input is tensor([18, 47, 56]), the target is: 57\n",
            "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
            "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
          ]
        }
      ],
      "source": [
        "# Define the block size, which is the length of the sequence\n",
        "block_size = 8\n",
        "\n",
        "# Extract the input sequence 'x' and target sequence 'y' from the training data\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1 : block_size + 1]\n",
        "\n",
        "# Iterate through each time step in the sequence\n",
        "for t in range(block_size):\n",
        "    # Create the context by selecting the input sequence up to time step 't'\n",
        "    context = x[:t + 1]\n",
        "\n",
        "    # Determine the target character at time step 't'\n",
        "    target = y[t]\n",
        "\n",
        "    # Print the context and corresponding target character\n",
        "    print(f\"When input is {context}, the target is: {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcWjs2CfmEhj",
        "outputId": "c98dfbad-f546-4f32-9aa1-7069a6c64ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "When input is [24], the target is: 43\n",
            "When input is [24, 43], the target is: 58\n",
            "When input is [24, 43, 58], the target is: 5\n",
            "When input is [24, 43, 58, 5], the target is: 57\n",
            "When input is [24, 43, 58, 5, 57], the target is: 1\n",
            "When input is [24, 43, 58, 5, 57, 1], the target is: 46\n",
            "When input is [24, 43, 58, 5, 57, 1, 46], the target is: 43\n",
            "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is: 39\n",
            "When input is [44], the target is: 53\n",
            "When input is [44, 53], the target is: 56\n",
            "When input is [44, 53, 56], the target is: 1\n",
            "When input is [44, 53, 56, 1], the target is: 58\n",
            "When input is [44, 53, 56, 1, 58], the target is: 46\n",
            "When input is [44, 53, 56, 1, 58, 46], the target is: 39\n",
            "When input is [44, 53, 56, 1, 58, 46, 39], the target is: 58\n",
            "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is: 1\n",
            "When input is [52], the target is: 58\n",
            "When input is [52, 58], the target is: 1\n",
            "When input is [52, 58, 1], the target is: 58\n",
            "When input is [52, 58, 1, 58], the target is: 46\n",
            "When input is [52, 58, 1, 58, 46], the target is: 39\n",
            "When input is [52, 58, 1, 58, 46, 39], the target is: 58\n",
            "When input is [52, 58, 1, 58, 46, 39, 58], the target is: 1\n",
            "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is: 46\n",
            "When input is [25], the target is: 17\n",
            "When input is [25, 17], the target is: 27\n",
            "When input is [25, 17, 27], the target is: 10\n",
            "When input is [25, 17, 27, 10], the target is: 0\n",
            "When input is [25, 17, 27, 10, 0], the target is: 21\n",
            "When input is [25, 17, 27, 10, 0, 21], the target is: 1\n",
            "When input is [25, 17, 27, 10, 0, 21, 1], the target is: 54\n",
            "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is: 39\n"
          ]
        }
      ],
      "source": [
        "# Set a manual seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Define batch size (number of independent sequences to process in parallel) and block size (maximum context length)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "# Function to get a batch of data\n",
        "def get_batch(split):\n",
        "    # Select the appropriate dataset based on the split (train or validation)\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Randomly select 'batch_size' starting positions within the dataset\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Extract input sequences 'x' and target sequences 'y' for the selected positions\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Get a batch of training data\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "# Print the shapes and contents of the input and target sequences in the batch\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "# Iterate through the batch dimension and time dimension to display inputs and targets\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"When input is {context.tolist()}, the target is: {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqebGyDjmEkW",
        "outputId": "14abc352-ca6a-4b10-a544-68aa2fad4afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits Shape: torch.Size([32, 65])\n",
            "Loss: tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "Generated Text: \n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Set a manual seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Define the Bigram Language Model class\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # 'idx' and 'targets' are both (B,T) tensors of integers\n",
        "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # 'idx' is a (B, T) tensor of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, C)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Create an instance of the Bigram Language Model with the specified vocabulary size\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# Compute logits and loss for the given input sequences\n",
        "logits, loss = m(xb, yb)\n",
        "print(\"Logits Shape:\", logits.shape)\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "# Generate text using the model starting from an empty context\n",
        "generated_text = decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())\n",
        "print(\"Generated Text:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nJqcQ7gmEmh"
      },
      "outputs": [],
      "source": [
        "# Create a PyTorch optimizer for training the model's parameters\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYlKgm92mEo8",
        "outputId": "e40722b0-99b4-4b3e-adf4-cf064023c9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Loss: 4.587916374206543\n"
          ]
        }
      ],
      "source": [
        "# Set the batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Loop for a specified number of training steps\n",
        "for steps in range(100):  # Increase the number of steps for better results...\n",
        "\n",
        "    # Sample a batch of training data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Evaluate the loss and perform gradient descent\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)  # Zero out gradients to avoid accumulation\n",
        "    loss.backward()  # Compute gradients using backpropagation\n",
        "    optimizer.step()  # Update model parameters using the optimizer\n",
        "\n",
        "# Print the final loss after training\n",
        "print(\"Final Loss:\", loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc2kOqgEmEsx",
        "outputId": "330e872d-ce7b-45bd-98cb-d0096e3c4aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text:\n",
            "\n",
            "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
            "3Q&sGlvHQ?mqSq-eON\n",
            "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
            "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
            "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
            "&WDdP!Ko,px\n",
            "x\n",
            "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
            "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
          ]
        }
      ],
      "source": [
        "# Generate text using the trained model starting from an empty context\n",
        "generated_text = decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuAgju7CmEyV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGkWRtGIpy-S",
        "outputId": "6914019c-effb-4a6f-e44c-0e0ee4c86b51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1c901611b0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hyperparameters for training and model architecture\n",
        "\n",
        "# Batch size: Number of independent sequences to process in parallel during training\n",
        "batch_size = 16\n",
        "\n",
        "# Block size: Maximum context length for predictions\n",
        "block_size = 32\n",
        "\n",
        "# Maximum number of training iterations\n",
        "max_iters = 5000\n",
        "\n",
        "# Interval for evaluation (printing progress, generating text, etc.)\n",
        "eval_interval = 100\n",
        "\n",
        "# Learning rate for optimizer\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Device for training (CPU or CUDA GPU if available)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Evaluation iterations (how often to evaluate the model)\n",
        "eval_iters = 200\n",
        "\n",
        "# Model architecture hyperparameters\n",
        "n_embd = 64     # Embedding dimension\n",
        "n_head = 4      # Number of attention heads\n",
        "n_layer = 4     # Number of layers in the model\n",
        "dropout = 0.0   # Dropout rate\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdBJNJXPpzB4"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdndCw-4pzFm"
      },
      "outputs": [],
      "source": [
        "# Find all unique characters in the 'text' variable and sort them\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "# Calculate the vocabulary size, which is the total number of unique characters\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create a mapping from characters to integers using dictionary comprehension\n",
        "# stoi: character-to-integer mapping\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create a mapping from integers to characters using dictionary comprehension\n",
        "# itos: integer-to-character mapping\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Define an encoder function that takes a string and outputs a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "\n",
        "# Define a decoder function that takes a list of integers and outputs a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBubeng6pzKX"
      },
      "outputs": [],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZlG2b66qVwI"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Generate a batch of input sequences (x) and target sequences (y) for training or validation.\n",
        "\n",
        "    Args:\n",
        "        split (str): 'train' for training data or 'val' for validation data.\n",
        "\n",
        "    Returns:\n",
        "        x (torch.Tensor): Batch of input sequences.\n",
        "        y (torch.Tensor): Batch of target sequences.\n",
        "\n",
        "    \"\"\"\n",
        "    # Select the appropriate dataset based on the split (train or validation)\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Randomly select 'batch_size' starting positions within the dataset\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Extract input sequences 'x' and target sequences 'y' for the selected positions\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    # Move the tensors to the specified device (CPU or GPU)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoRRCMhLqV0I"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()  # Decorator to disable gradient tracking during inference\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    Estimate the loss of the model on the training and validation datasets.\n",
        "\n",
        "    Returns:\n",
        "        out (dict): A dictionary containing mean loss values for 'train' and 'val' splits.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval()  # Set the model to evaluation mode (no gradient computation)\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        # Calculate the mean loss for this split\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train()  # Set the model back to training mode\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6GhA_R8qV6I"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self-attention. \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear transformations for key, query, and value\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # Register a lower triangular mask (used for masking future positions)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # Key projection (B, T, head_size)\n",
        "        q = self.query(x) # Query projection (B, T, head_size)\n",
        "\n",
        "        # Compute attention scores (\"affinities\") between queries and keys\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, T)\n",
        "\n",
        "        # Apply a lower triangular mask to prevent attending to future positions\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        # Apply softmax to obtain attention weights\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # Apply dropout to attention weights\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Perform weighted aggregation of values\n",
        "        v = self.value(x) # Value projection (B, T, head_size)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy9hfEyeqV8z"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of self-attention in parallel. \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a list of attention heads\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "        # Linear transformation for projection\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply each attention head in parallel and concatenate the results\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads * head_size)\n",
        "\n",
        "        # Apply linear projection\n",
        "        out = self.dropout(self.proj(out)) # (B, T, n_embd)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8pwG7gVqV_t"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" A simple linear layer followed by a non-linearity and dropout. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define a sequential network with linear layers, ReLU activation, and dropout\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),  # Linear layer with 4 times the embedding dimension\n",
        "            nn.ReLU(),  # ReLU activation function\n",
        "            nn.Linear(4 * n_embd, n_embd),  # Linear layer to project back to the embedding dimension\n",
        "            nn.Dropout(dropout),  # Dropout layer for regularization\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the defined network to the input tensor 'x'\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ur0CJWQrjmj"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        \"\"\"\n",
        "        Initialize a Transformer block.\n",
        "\n",
        "        Args:\n",
        "            n_embd (int): Embedding dimension.\n",
        "            n_head (int): Number of attention heads.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Calculate the size of each attention head\n",
        "        head_size = n_embd // n_head\n",
        "\n",
        "        # Multi-head self-attention layer\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "\n",
        "        # Feedforward neural network layer\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "        # Layer normalization layers\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the Transformer block.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after processing through the block.\n",
        "        \"\"\"\n",
        "        # Apply self-attention layer followed by layer normalization\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "\n",
        "        # Apply feedforward neural network layer followed by layer normalization\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9sZkujXrjqY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Token embedding table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # Position embedding table\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # Stack of transformer blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        # Layer normalization for the final output\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n",
        "\n",
        "        # Linear layer for language modeling prediction\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensors of integers\n",
        "\n",
        "        # Token embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "\n",
        "        # Position embeddings\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "\n",
        "        # Add token and position embeddings\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "\n",
        "        # Apply the stack of transformer blocks\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "\n",
        "        # Apply layer normalization to the output\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "\n",
        "        # Generate logits for language modeling\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      \"\"\"\n",
        "      Generate new text based on the given initial context.\n",
        "\n",
        "      Args:\n",
        "          idx (torch.Tensor): Initial context as a tensor of indices (B, T).\n",
        "          max_new_tokens (int): Maximum number of new tokens to generate.\n",
        "\n",
        "      Returns:\n",
        "          torch.Tensor: Tensor of generated indices (B, T+max_new_tokens).\n",
        "\n",
        "      \"\"\"\n",
        "      # Iterate for 'max_new_tokens' steps to generate new tokens\n",
        "      for _ in range(max_new_tokens):\n",
        "          # Crop the context to the last 'block_size' tokens\n",
        "          idx_cond = idx[:, -block_size:]\n",
        "\n",
        "          # Get predictions from the model for the current context\n",
        "          logits, loss = self(idx_cond)\n",
        "\n",
        "          # Focus only on the last time step\n",
        "          logits = logits[:, -1, :]  # Shape: (B, C)\n",
        "\n",
        "          # Apply softmax to obtain probabilities\n",
        "          probs = F.softmax(logits, dim=-1)  # Shape: (B, C)\n",
        "\n",
        "          # Sample from the probability distribution to get the next token\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)  # Shape: (B, 1)\n",
        "\n",
        "          # Append the sampled index to the running sequence\n",
        "          idx = torch.cat((idx, idx_next), dim=1)  # Shape: (B, T+1)\n",
        "\n",
        "      return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryb1zg1arjtj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNGLFp5ErkBl",
        "outputId": "3ffc30fe-8bf9-489f-e550-2bac2b1d119c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5059\n",
            "step 300: train loss 2.4196, val loss 2.4338\n",
            "step 400: train loss 2.3503, val loss 2.3565\n",
            "step 500: train loss 2.2966, val loss 2.3129\n",
            "step 600: train loss 2.2410, val loss 2.2500\n",
            "step 700: train loss 2.2051, val loss 2.2191\n",
            "step 800: train loss 2.1640, val loss 2.1874\n",
            "step 900: train loss 2.1251, val loss 2.1515\n",
            "step 1000: train loss 2.1023, val loss 2.1291\n",
            "step 1100: train loss 2.0699, val loss 2.1192\n",
            "step 1200: train loss 2.0375, val loss 2.0797\n",
            "step 1300: train loss 2.0259, val loss 2.0647\n",
            "step 1400: train loss 1.9924, val loss 2.0362\n",
            "step 1500: train loss 1.9700, val loss 2.0304\n",
            "step 1600: train loss 1.9631, val loss 2.0476\n",
            "step 1700: train loss 1.9412, val loss 2.0131\n",
            "step 1800: train loss 1.9097, val loss 1.9960\n",
            "step 1900: train loss 1.9101, val loss 1.9882\n",
            "step 2000: train loss 1.8867, val loss 1.9976\n",
            "step 2100: train loss 1.8720, val loss 1.9754\n",
            "step 2200: train loss 1.8588, val loss 1.9606\n",
            "step 2300: train loss 1.8542, val loss 1.9525\n",
            "step 2400: train loss 1.8424, val loss 1.9464\n",
            "step 2500: train loss 1.8173, val loss 1.9455\n",
            "step 2600: train loss 1.8256, val loss 1.9388\n",
            "step 2700: train loss 1.8116, val loss 1.9350\n",
            "step 2800: train loss 1.8056, val loss 1.9214\n",
            "step 2900: train loss 1.8040, val loss 1.9300\n",
            "step 3000: train loss 1.7974, val loss 1.9205\n",
            "step 3100: train loss 1.7694, val loss 1.9157\n",
            "step 3200: train loss 1.7539, val loss 1.9115\n",
            "step 3300: train loss 1.7571, val loss 1.9071\n",
            "step 3400: train loss 1.7531, val loss 1.8954\n",
            "step 3500: train loss 1.7368, val loss 1.8918\n",
            "step 3600: train loss 1.7274, val loss 1.8884\n",
            "step 3700: train loss 1.7301, val loss 1.8819\n",
            "step 3800: train loss 1.7210, val loss 1.8938\n",
            "step 3900: train loss 1.7260, val loss 1.8750\n",
            "step 4000: train loss 1.7122, val loss 1.8554\n",
            "step 4100: train loss 1.7129, val loss 1.8717\n",
            "step 4200: train loss 1.7041, val loss 1.8634\n",
            "step 4300: train loss 1.6986, val loss 1.8434\n",
            "step 4400: train loss 1.7052, val loss 1.8605\n",
            "step 4500: train loss 1.6881, val loss 1.8467\n",
            "step 4600: train loss 1.6849, val loss 1.8318\n",
            "step 4700: train loss 1.6833, val loss 1.8449\n",
            "step 4800: train loss 1.6686, val loss 1.8472\n",
            "step 4900: train loss 1.6719, val loss 1.8425\n",
            "step 4999: train loss 1.6619, val loss 1.8215\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oB-7TpL2qWDG",
        "outputId": "cbea0927-24e7-453f-c6fc-378f26e44ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "And they bride will to lay be madie;\n",
            "Thou but take O-dam the change:\n",
            "Warth full him tother dilth ane away, my fears,\n",
            "You have was them of is heart mile,\n",
            "You, and if ensmy contlatist, drov the does me now that\n",
            "just, lesing that.\n",
            "His my now, you up; and the tyby love.\n",
            "In Bodiet, and whom\n",
            "that demperakenous, so what evily well my\n",
            "Murtus censurence of him the reshep and thrust for to imper my monte in Mont,\n",
            "To fight? gry of thy hourb! stiddy as\n",
            "ards bearing her broint must are no Runnts\n",
            "Infortuce will me not be arm.\n",
            "You contrantymes have myse.-\n",
            "And fortwerle madam them may in son, live body.\n",
            "\n",
            "Think you:\n",
            "It stay might. \n",
            "CLAMENCE:\n",
            "My whilesse everew in movet, if Cassce of's counted;\n",
            "How what make you fear tals: the gold my sun?\n",
            "What, loudy forgor man our him.\n",
            "I will were but with some. Povinly Ford the welcont.\n",
            "\n",
            "QUEEN FIDILIZ:\n",
            "No?\n",
            "Their him the not.\n",
            "\n",
            "POLIXENENE:\n",
            "But to me, God no now the summe wip.\n",
            "\n",
            "GROMPEO:\n",
            "Conguit, bruke this belike, on so han the bodiet.\n",
            "\n",
            "CORIOLANUS:\n",
            "Till the;\n",
            "you wellseers I am with you,\n",
            "For I hust no where Mustconce, do wind that I am nobly.\n",
            "\n",
            "BRUSTHORD:\n",
            "O, wenterings so me worting.\n",
            "\n",
            "GRUMIO:\n",
            "O thus favour now,\n",
            "An bear was all beenIn\n",
            "Before and to the sever--and.\n",
            "In to dot me, to liberfeleing breamn'd my have\n",
            "epince, if that jutcey's leve,\n",
            "That Tumselfly there's little ofjess the vown;\n",
            "Maughter armied maste love in stide belothy dong'd the not.\n",
            "\n",
            "BENVOLIO:\n",
            "Well cavonzy to I have must aboe;\n",
            "I now, I thinke numt om Three teny, delelige,\n",
            "And yet our son one old, we\n",
            "ell sment on you; and plock, say, as If have to kavidess corby?\n",
            "Then eteep; upose worth\n",
            "But arm one wall preven him there.\n",
            "\n",
            "BUCKINGHARD\n",
            "\n",
            "IVIRHAMIUS:\n",
            "Why, unere to-marrow thy sathe court his in on\n",
            "some no, God the have blay not, these wife it:\n",
            "The that hear I, thou with art, lives?\n",
            "\n",
            "LARY:\n",
            "Our while with you\n",
            "That I horrtw'd will theirs is.\n",
            "Why, I would I drue, and was father,--\n",
            "'Tensis, thy promb, many and sentry talbatt.\n",
            "\n",
            "PORDINCE:\n",
            "Why Riparding:\n",
            "In is shown's fortunds, but whom the brike our all\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UMA0sCgvmE1L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}